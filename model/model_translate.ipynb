{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c7e79c",
   "metadata": {},
   "source": [
    "# -------------Giai ƒëo·∫°n 1: X·ª≠ l√≠ d·ªØ li·ªáu-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d014f",
   "metadata": {},
   "source": [
    "* Khai b√°o th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ed6e7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce8de3",
   "metadata": {},
   "source": [
    "# 1. Load Tokenizer (Spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "34b6cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"T√°ch t·ª´ ti·∫øng Anh, ƒë·∫£o ng∆∞·ª£c n·∫øu c·∫ßn (nh∆∞ng l√∫c n√†y c·ª© ƒë·ªÉ xu√¥i ƒë√™)\"\"\"\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    \"\"\"T√°ch t·ª´ ti·∫øng Ph√°p\"\"\"\n",
    "    return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f4e3b",
   "metadata": {},
   "source": [
    "# 2. X√¢y d·ª±ng b·ªô t·ª´ v·ª±ng (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9cfad0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        # 4 token ƒë·∫∑c bi·ªát b·∫Øt bu·ªôc ph·∫£i c√≥\n",
    "        self.itos = {0: \"<unk>\", 1: \"<pad>\", 2: \"<sos>\", 3: \"<eos>\"}\n",
    "        self.stoi = {\"<unk>\": 0, \"<pad>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list, tokenizer):\n",
    "        frequencies = Counter()\n",
    "        idx = 4 # B·∫Øt ƒë·∫ßu t·ª´ 4 v√¨ 0-3 ƒë√£ d√πng cho token ƒë·∫∑c bi·ªát\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                # Ch·ªâ th√™m t·ª´ xu·∫•t hi·ªán ƒë·ªß nhi·ªÅu (tr√°nh r√°c)\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self, text, tokenizer):\n",
    "        \"\"\"Chuy·ªÉn c√¢u text th√†nh list c√°c s·ªë (indices)\"\"\"\n",
    "        tokenized_text = tokenizer(text)\n",
    "        \n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496861d4",
   "metadata": {},
   "source": [
    "# 3. Class Dataset Ch√≠nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "93cb4219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi30kDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', src_vocab=None, trg_vocab=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        \n",
    "        # ƒê·ªçc file raw\n",
    "        en_path = os.path.join(root_dir, f'{mode}.en')\n",
    "        fr_path = os.path.join(root_dir, f'{mode}.fr')\n",
    "        \n",
    "        with open(en_path, 'r', encoding='utf-8') as f:\n",
    "            self.source_sentences = [line.strip() for line in f.readlines()]\n",
    "            \n",
    "        with open(fr_path, 'r', encoding='utf-8') as f:\n",
    "            self.target_sentences = [line.strip() for line in f.readlines()]\n",
    "            \n",
    "        # X√¢y d·ª±ng ho·∫∑c d√πng l·∫°i vocab\n",
    "        if src_vocab is None or trg_vocab is None:\n",
    "            print(f\" ƒêang x√¢y d·ª±ng Vocabulary t·ª´ t·∫≠p {mode}...\")\n",
    "            self.src_vocab = Vocabulary()\n",
    "            self.trg_vocab = Vocabulary()\n",
    "            \n",
    "            self.src_vocab.build_vocabulary(self.source_sentences, tokenize_en)\n",
    "            self.trg_vocab.build_vocabulary(self.target_sentences, tokenize_fr)\n",
    "            print(f\" Vocab xong! Ti·∫øng Anh: {len(self.src_vocab)} t·ª´, Ti·∫øng Ph√°p: {len(self.trg_vocab)} t·ª´.\")\n",
    "        else:\n",
    "            self.src_vocab = src_vocab\n",
    "            self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src_text = self.source_sentences[index]\n",
    "        trg_text = self.target_sentences[index]\n",
    "\n",
    "        # Chuy·ªÉn text sang s·ªë: <sos> ...c√¢u... <eos>\n",
    "        src_numericalized = [self.src_vocab.stoi[\"<sos>\"]]\n",
    "        src_numericalized += self.src_vocab.numericalize(src_text, tokenize_en)\n",
    "        src_numericalized += [self.src_vocab.stoi[\"<eos>\"]]\n",
    "\n",
    "        trg_numericalized = [self.trg_vocab.stoi[\"<sos>\"]]\n",
    "        trg_numericalized += self.trg_vocab.numericalize(trg_text, tokenize_fr)\n",
    "        trg_numericalized += [self.trg_vocab.stoi[\"<eos>\"]]\n",
    "\n",
    "        return torch.tensor(src_numericalized), torch.tensor(trg_numericalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1a5c5",
   "metadata": {},
   "source": [
    "# 4. H√†m Collate (X·ª≠ l√Ω Batch - QUAN TR·ªåNG NH·∫§T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "628aa53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch l√† list c√°c c·∫∑p (src, trg) t·ª´ __getitem__\n",
    "        \n",
    "        # S·∫Øp x·∫øp batch theo ƒë·ªô d√†i c√¢u ngu·ªìn gi·∫£m d·∫ßn (B·∫Øt bu·ªôc cho pack_padded_sequence)\n",
    "        batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "        \n",
    "        src_batch, trg_batch = zip(*batch)\n",
    "        \n",
    "        # Padding: th√™m s·ªë 1 (<pad>) v√†o cho b·∫±ng ƒë·ªô d√†i\n",
    "        src_padded = pad_sequence(src_batch, padding_value=self.pad_idx, batch_first=False) # Shape: [src_len, batch_size]\n",
    "        trg_padded = pad_sequence(trg_batch, padding_value=self.pad_idx, batch_first=False) # Shape: [trg_len, batch_size]\n",
    "        \n",
    "        return src_padded, trg_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ecf6d",
   "metadata": {},
   "source": [
    "# --- H√ÄM TEST TH·ª¨ XEM (Ch·∫°y c√°i n√†y ƒë·ªÉ xem code c√≥ crash kh√¥ng ta) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "817b0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ƒêang x√¢y d·ª±ng Vocabulary t·ª´ t·∫≠p train...\n",
      " Vocab xong! Ti·∫øng Anh: 5893 t·ª´, Ti·∫øng Ph√°p: 6470 t·ª´.\n",
      "\n",
      " Ki·ªÉm tra 1 Batch:\n",
      "Shape Source: torch.Size([24, 32])\n",
      "Shape Target: torch.Size([31, 32])\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ƒê∆∞·ªùng d·∫´n data clean c·ªßa tui\n",
    "    DATA_DIR = \"../data_clean\" \n",
    "    \n",
    "    # 1. T·∫°o Dataset\n",
    "    train_dataset = Multi30kDataset(DATA_DIR, mode='train')\n",
    "    val_dataset = Multi30kDataset(DATA_DIR, mode='val', src_vocab=train_dataset.src_vocab, trg_vocab=train_dataset.trg_vocab)\n",
    "    \n",
    "    # 2. T·∫°o DataLoader\n",
    "    pad_idx = train_dataset.src_vocab.stoi[\"<pad>\"]\n",
    "    # d√πng cpu n√™n ƒë·ªÉ num_workers=0 cho l√†nh, batch_size nh·ªè th√¥i\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32, \n",
    "        num_workers=0, \n",
    "        shuffle=True, \n",
    "        collate_fn=Collate(pad_idx=pad_idx)\n",
    "    )\n",
    "    \n",
    "    # 3. L·∫•y th·ª≠ 1 batch ra soi\n",
    "    print(\"\\n Ki·ªÉm tra 1 Batch:\")\n",
    "    for src, trg in train_loader:\n",
    "        print(f\"Shape Source: {src.shape}\") # Mong ƒë·ª£i: [Max_Len_Src, 32]\n",
    "        print(f\"Shape Target: {trg.shape}\") # Mong ƒë·ª£i: [Max_Len_Trg, 32]\n",
    "        print(\"Done!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe49467",
   "metadata": {},
   "source": [
    "# --------------GIAI ƒêO·∫†N 2: X√ÇY D·ª∞NG MODEL------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ba78d",
   "metadata": {},
   "source": [
    "* Khai b√°o th√™m th∆∞ vi·ªán "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2860d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c1cdd",
   "metadata": {},
   "source": [
    "# 1. M√£ h√≥a l·ªõp Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7d587e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Chuy·ªÉn t·ª´ (s·ªë) sang vector\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        # LSTM: input_size=emb_dim, hidden_size=hid_dim\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src shape: [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded shape: [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        # outputs: ch·ª©a hidden state c·ªßa t·∫•t c·∫£ c√°c time step (d√πng cho Attention sau n√†y)\n",
    "        # hidden, cell: tr·∫°ng th√°i ·∫©n cu·ªëi c√πng (Context Vector) -> C·∫ßn cho Decoder\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d2b24",
   "metadata": {},
   "source": [
    "# 2. Gi·∫£i m√£ (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0e6a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding cho Target (Ti·∫øng Ph√°p)\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        # Linear layer ƒë·ªÉ d·ª± ƒëo√°n t·ª´ ti·∫øp theo\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input shape: [batch size] (ch·ªâ 1 t·ª´ t·∫°i 1 th·ªùi ƒëi·ªÉm)\n",
    "        # hidden, cell: context vector t·ª´ b∆∞·ªõc tr∆∞·ªõc\n",
    "        \n",
    "        input = input.unsqueeze(0) \n",
    "        # input shape: [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded shape: [1, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output shape: [1, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction shape: [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef286e6c",
   "metadata": {},
   "source": [
    "# 3. T·∫°o m√¥ h√¨nh Seq2Seq k·∫øt h·ª£p Encoder v√† Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d1a94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTranslationModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        # Check size cho ch·∫Øc c√∫\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [src len, batch size]\n",
    "        # trg: [trg len, batch size]\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # Tensor ch·ª©a k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # 1. M√£ h√≥a c√¢u ngu·ªìn (Encoder) -> L·∫•y context vector (hidden, cell)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # 2. ƒê·∫ßu v√†o ƒë·∫ßu ti√™n cho Decoder l√† token <sos> (Start of Sentence)\n",
    "        input = trg[0, :]\n",
    "        \n",
    "        # 3. V√≤ng l·∫∑p gi·∫£i m√£ t·ª´ng t·ª´ (Decoder)\n",
    "        for t in range(1, trg_len):\n",
    "            # Ch·∫°y 1 b∆∞·ªõc decoder\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            # L∆∞u d·ª± ƒëo√°n v√†o tensor outputs\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # Quy·∫øt ƒë·ªãnh Teacher Forcing:\n",
    "            # N·∫øu random < ratio -> d√πng t·ª´ th·∫≠t (target) l√†m input ti·∫øp theo\n",
    "            # Ng∆∞·ª£c l·∫°i -> d√πng t·ª´ d·ª± ƒëo√°n cao nh·∫•t (top1) l√†m input ti·∫øp theo\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86949213",
   "metadata": {},
   "source": [
    "# --- TEST TH·ª¨ MODEL ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f667715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ƒêang ch·∫°y th·ª≠ model qua 1 v√≤ng forward...\n",
      " Output Shape: torch.Size([26, 32, 1000])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Gi·∫£ l·∫≠p tham s·ªë\n",
    "    INPUT_DIM = 1000\n",
    "    OUTPUT_DIM = 1000\n",
    "    ENC_EMB_DIM = 256\n",
    "    DEC_EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    N_LAYERS = 2\n",
    "    ENC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0.5\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = MyTranslationModel(enc, dec, DEVICE).to(DEVICE)\n",
    "\n",
    "    # Gi·∫£ l·∫≠p input (nh∆∞ output c·ªßa Dataset n√£y ch·∫°y)\n",
    "    src = torch.randint(0, 1000, (24, 32)).to(DEVICE) # [Src Len, Batch Size]\n",
    "    trg = torch.randint(0, 1000, (26, 32)).to(DEVICE) # [Trg Len, Batch Size]\n",
    "\n",
    "    print(\" ƒêang ch·∫°y th·ª≠ model qua 1 v√≤ng forward...\")\n",
    "    output = model(src, trg)\n",
    "    print(f\" Output Shape: {output.shape}\") \n",
    "    # Mong ƒë·ª£i: [26, 32, 1000] -> [Trg Len, Batch Size, Output Dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b70a445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è ƒêang ch·∫°y tr√™n: cpu\n",
      "‚è≥ ƒêang load d·ªØ li·ªáu...\n",
      " ƒêang x√¢y d·ª±ng Vocabulary t·ª´ t·∫≠p train...\n",
      " Vocab xong! Ti·∫øng Anh: 5893 t·ª´, Ti·∫øng Ph√°p: 6470 t·ª´.\n",
      "‚úÖ Data ngon l√†nh! Train: 29000, Val: 1014\n",
      "\n",
      "üöÄ B·∫Øt ƒë·∫ßu Train 10 epochs...\n",
      "   Batch 0/907 | Loss: 8.775\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 157\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m    155\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 157\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, train_loader, optimizer, criterion, CLIP)\n\u001b[0;32m    158\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[0;32m    160\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[89], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     77\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[1;32m---> 80\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Clip gradient ƒë·ªÉ tr√°nh b√πng n·ªï\u001b[39;00m\n\u001b[0;32m     83\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    627\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[0;32m    355\u001b[0m     tensors,\n\u001b[0;32m    356\u001b[0m     grad_tensors_,\n\u001b[0;32m    357\u001b[0m     retain_graph,\n\u001b[0;32m    358\u001b[0m     create_graph,\n\u001b[0;32m    359\u001b[0m     inputs_tuple,\n\u001b[0;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    362\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import m·∫•y c√°i class n√£y gi·ªù m√¨nh vi·∫øt (Gi·∫£ s·ª≠ m√†y ƒë·ªÉ chung 1 file ho·∫∑c import t·ª´ file kh√°c)\n",
    "# from data_loader import Multi30kDataset, Collate\n",
    "# from model import Encoder, Decoder, Seq2Seq\n",
    "\n",
    "# --- C·∫§U H√åNH (CONFIG) ---\n",
    "DATA_DIR = \"../data_clean\" # Folder ch·ª©a data chu·∫©n\n",
    "BATCH_SIZE = 32         # CPU y·∫øu th√¨ ƒë·ªÉ 32 ho·∫∑c 16 th√¥i\n",
    "N_EPOCHS = 10           # Ch·∫°y th·ª≠ 5-10 epoch xem sao\n",
    "CLIP = 1.0              # C·∫Øt gradient ƒë·ªÉ kh√¥ng b·ªã l·ªói\n",
    "LR = 0.001              # T·ªëc ƒë·ªô h·ªçc\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è ƒêang ch·∫°y tr√™n: {DEVICE}\")\n",
    "\n",
    "# --- 1. CHU·∫®N B·ªä D·ªÆ LI·ªÜU ---\n",
    "print(\"‚è≥ ƒêang load d·ªØ li·ªáu...\")\n",
    "train_dataset = Multi30kDataset(DATA_DIR, mode='train')\n",
    "val_dataset = Multi30kDataset(DATA_DIR, mode='val', src_vocab=train_dataset.src_vocab, trg_vocab=train_dataset.trg_vocab)\n",
    "# test_dataset = Multi30kDataset(DATA_DIR, mode='test', src_vocab=train_dataset.src_vocab, trg_vocab=train_dataset.trg_vocab)\n",
    "\n",
    "pad_idx = train_dataset.src_vocab.stoi[\"<pad>\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=Collate(pad_idx), num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=Collate(pad_idx), num_workers=0)\n",
    "\n",
    "print(f\"‚úÖ Data ngon l√†nh! Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# --- 2. KH·ªûI T·∫†O MODEL ---\n",
    "INPUT_DIM = len(train_dataset.src_vocab)\n",
    "OUTPUT_DIM = len(train_dataset.trg_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = MyTranslationModel(enc, dec, DEVICE).to(DEVICE)\n",
    "\n",
    "# Kh·ªüi t·∫°o tr·ªçng s·ªë (Weight Initialization) - Gi√∫p model h·ªçc nhanh h∆°n\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# B·ªè qua loss c·ªßa token <pad>\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# --- 3. H√ÄM TRAIN & EVALUATE ---\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # output: [trg len, batch size, output dim]\n",
    "        output = model(src, trg) \n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # B·ªè token <sos> ƒë·∫ßu ti√™n khi t√≠nh loss\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradient ƒë·ªÉ tr√°nh b√πng n·ªï\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Log nh·∫π c√°i ƒë·ªÉ bi·∫øt m√°y kh√¥ng b·ªã treo (m·ªói 50 batch b√°o 1 l·∫ßn)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"   Batch {i}/{len(iterator)} | Loss: {loss.item():.3f}\")\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            output = model(src, trg, teacher_forcing_ratio=0) # T·∫Øt teacher forcing khi val/test\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# --- 4. H√ÄM D·ªäCH TH·ª¨ (L·∫§Y ƒêI·ªÇM) ---\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Tokenize & Numericalize\n",
    "    if isinstance(sentence, str):\n",
    "        import spacy\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "        tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "    src_indexes = [src_vocab.stoi.get(token, src_vocab.stoi[\"<unk>\"]) for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device) # [len, 1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encoder\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    # Decoder\n",
    "    trg_indexes = [trg_vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device) # [1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_vocab.itos[i] for i in trg_indexes]\n",
    "    return trg_tokens[1:-1] # B·ªè sos, eos\n",
    "\n",
    "# --- 5. V√íNG L·∫∂P CH√çNH (MAIN LOOP) ---\n",
    "if __name__ == \"__main__\":\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f\"\\nüöÄ B·∫Øt ƒë·∫ßu Train {N_EPOCHS} epochs...\")\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, val_loader, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(valid_loss)\n",
    "        \n",
    "        # L∆∞u checkpoint x·ªãn nh·∫•t (Y√äU C·∫¶U C·ª¶A ƒê·ªÄ)\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"   üíæ ƒê√£ l∆∞u model t·ªët nh·∫•t (Val Loss: {valid_loss:.3f})\")\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "    # --- 6. V·∫º BI·ªÇU ƒê·ªí & TEST TH·ª¨ ---\n",
    "    print(\"\\nüìä ƒêang v·∫Ω bi·ªÉu ƒë·ªì Loss...\")\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss_chart.png') # L∆∞u ·∫£nh ƒë·ªÉ n·ªôp b√°o c√°o\n",
    "    plt.show()\n",
    "\n",
    "    # Load l·∫°i model ngon nh·∫•t ƒë·ªÉ test\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    print(\"\\nüí¨ D·ªãch th·ª≠ 1 c√¢u:\")\n",
    "    sentence = \"A man is walking a dog.\"\n",
    "    translation = translate_sentence(sentence, train_dataset.src_vocab, train_dataset.trg_vocab, model, DEVICE)\n",
    "    print(f\"Src: {sentence}\")\n",
    "    print(f\"Trg: {' '.join(translation)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
